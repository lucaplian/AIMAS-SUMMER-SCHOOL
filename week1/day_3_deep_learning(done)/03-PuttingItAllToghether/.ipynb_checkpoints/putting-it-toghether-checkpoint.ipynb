{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate, Test and Infer\n",
    "## Putting It All Together\n",
    "\n",
    "Thus far we talked about building, training, validating, testing and inference with deep neural networks on separate examples. While the discussions occurred in the context of fully connected networks, in practice all these tasks are performed for any machine learning model. So the goal here is to put them all together for neural networks as you would normally do in practice.\n",
    "\n",
    "For this we will again use the FashionMNIST dataset, so without further ado here are the usual imports you will need: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# This will be explained in future notebooks, but feel free to look ahead in the documation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the first step you need to tackle is to implement the functionality that does the work required to prepare the data for your model. Since the goal is to do this in a generalized manner, you need define a function that outputs the loaders used throughout the hole process, namely: train, validate and test.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Implement in PyTorch the `train_valid_split` function that creates the *train/validation loaders based on:\n",
    "- dataset - the dataset to splittrain_valid_split\n",
    "- valid_size - the parameter that specifies the split threshold \n",
    "- batch_size - the size of the batch\n",
    "\n",
    "**Note:** Since FashionMNIST already has a dedicated test set directly available through torchvision, we don't need to separate it out from the train set. However, in general you will have all data in a single dataset. Hence, you would need to implement a 3 way split for training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.1. Implement train_valid_split()\n",
    "\n",
    "def train_valid_split(dataset, valid_size = 0.2, batch_size = 32):\n",
    "\n",
    "    \n",
    "    train_loader = None\n",
    "    valid_loader = None\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step you have to implement the neural network architecture of your model. While in the previous lab you directly created the network architecture using the `torch.nn` API, here you have to do it in a more general manner. Considering the fact that neural networks are composed of many different blocks (which themselves are composed of different primitives such as layers, activations, normalization and so on), a sensible thing to do is to define a network block making function.  \n",
    "\n",
    "\n",
    "For example, last time in the architecture from exercise 4 (see Figure below) each hidden layer was followed by a ReLu activation function, while the output layer was followed by a softmax activation to obtain the class probabilities. Moreover, after each hidden layer you applied dropout, except for the output layer.\n",
    "\n",
    "<img src=\"res/fashion-nn.png\" width=500px>\n",
    "\n",
    "You can structure this in either a hidden block - which combines a linear layer, ReLu and dropout - or an output block - which combines a linear layer with softmax. Hence, generally you can implement a `make_block` function that outputs the appropriate network block based on a selection parameter. Of course, here our parameter will be binary between hidden and output (True/False). However, generally you will have many types of hidden blocks, so this parameter may have an enumeration of options.\n",
    "\n",
    "Implement the `make_block()` function following the TODO's bellow.\n",
    "\n",
    "**Note:** [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html), [nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax) and [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) are useful here. Also, recall that you can combine neural network operations in a sequence using [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2.1. Implement the hidden and output blocks\n",
    "def make_block(input_dim, output_dim, p_dropout = 0.2, final_layer=False):\n",
    "    if not final_layer:\n",
    "        return nn.Sequential(\n",
    "             # Complete here\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            # Complete here\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2.2. Use make block to implement the network architecture\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(Network, self).__init__()\n",
    "        # Complete here\n",
    "        net_blocks = None\n",
    "        self.net = nn.Sequential(*net_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "         # Complete here\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.net.__repr__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.net.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Most often, training a neural network model takes quite a long time and its prone to a number learning problems with respect to generalization. Thus, you usually want save the model parameters that achieve the best (or a few top best) validation results. This will let you reuse these parameters at test/inference time or provide model parameter initialization points for architecture tweaks you may do latter on. So, in this exercise implement saving/loading functions for your the model you defined above.\n",
    "\n",
    "Follow the TODO's below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3.1. Implement the function to save your model\n",
    "def save_model(model, checkpoint='./checkpoints/checkpoint.pth'):\n",
    "    \n",
    "    # TODO 3.1.a) Create a model dictionary out of all its children (name, layer) pairs\n",
    "    checkpoint_model = None\n",
    "\n",
    "    # TODO 3.1.b) Create a dictionary to save the model and its state\n",
    "    checkpoint_dict = None\n",
    "    \n",
    "    # TODO 3.1.c) Save the dictonary a file\n",
    "    \n",
    "\n",
    "# TODO 3.2. Implement the function to load your model\n",
    "def load_model(model = None, checkpoint='./checkpoints/checkpoint.pth'):\n",
    "    # TODO.3.2.a) load the model from the file\n",
    "    checkpoint_dict = None\n",
    "    \n",
    "    if model is None:\n",
    "        # TODO 3.2. b) use nn.Sequential object to re-create the model\n",
    "        model = None\n",
    "    \n",
    "    # TODO: 3.2.c) Load the state in your model\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "In order to implement the training loop we have to define several components, namely:\n",
    "1. hyper-parameters specific to the training process you want to implement, such as batch size, learning rate, and so on.\n",
    "2. prepare the data loaders based on your dataset with appropriate transformations.\n",
    "3. instantiate the architecture you want to train.\n",
    "4. define the criterion (i.e. loss function) and the optimizer you want to use.\n",
    "\n",
    "Having completed all these steps you can then implement the actual training process in a function in order to reuse it as needed. So, bellow you'll start with the hyper-parameters definition. \n",
    "\n",
    "**Note:** You can chose any values you see fit for these hyper-parameters, but do it wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.1. Define the hyper-parameters\n",
    "batch_size = None\n",
    "valid_size = None\n",
    "\n",
    "input_dim = None\n",
    "hidden_dims = [None]\n",
    "output_dim = None\n",
    "\n",
    "epochs = None\n",
    "learning_rate = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the position to create the loaders, i.e. train/valid/test loaders. To do this implement the following steps:\n",
    "- download the FashionMNIST train and test sets, in `./datasets/`, while applying the appropriate transformation \n",
    "- call `train_valid_test_split()` to create train/valid loaders\n",
    "- create the test set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.2.a) Create the apropriate transformations\n",
    "transform = None\n",
    "\n",
    "# TODO 4.2.b) Get the training set and create train/validate loaders \n",
    "train_set = None\n",
    "train_loader, valid_loader = None\n",
    "\n",
    "# TODO 4.2.c) Get the test set and create it's loader\n",
    "test_set = None\n",
    "test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the last pieces you need for training are the model, the loss function and the SGD (based) optimizer. Here we deal with multi-class classification, so the criterion you need to use is the *negative log loss* `NLLLoss()`. Whereas, for the optimizer use `Adam()` such that the training is faster and more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.3. Create the model, criterion and optimizer\n",
    "model = None\n",
    "criterion = None\n",
    "optimizer = None\n",
    "\n",
    "# Comment this line out if you have any trouble\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now you have all the pieces and its time to put them together. In order to this you need to implement train/validate steps for training, while keeping track of the training/validation losses and saving the best check point. \n",
    "\n",
    "Follow the TODO's and comments below to complete these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.4. Implement the training function\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs=10):\n",
    "    \n",
    "    # Keeping track variabiles\n",
    "    best_valid_loss = np.Inf\n",
    "    train_losses, valid_losses = [], []\n",
    "    \n",
    "    # The training loop:\n",
    "    for epoch in range(epochs):\n",
    "        # Initializing the training loss for this epoch\n",
    "        training_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "\n",
    "            # Comment this line out if you have any trouble\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # TODO 4.4.a) Implement the actual training for this batch\n",
    "            loss = None\n",
    "            \n",
    "            # TODO 4.4.b) Keep track of invidual batch losses in this epoch.\n",
    "            training_loss = None\n",
    "\n",
    "        \n",
    "        # Intializing the validation loss and accuracy for this epoch\n",
    "        valid_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        # Turning off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images, labels in valid_loader:\n",
    "                # Comment this line out if you have any trouble\n",
    "                images, labels = images.to(device), labels.to(device) \n",
    "                \n",
    "                # TODO 4.4.c) Implement the actual validation for this batch\n",
    "                log_ps = None\n",
    "                valid_loss += None    \n",
    "                \n",
    "                # TODO 4.4.d) Compute the accuracy\n",
    "                ps = None\n",
    "                top_p, top_class = None\n",
    "                accuracy += None\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # TODO 4.4.e) Save the best model thus far\n",
    "                    \n",
    "        # TODO 4.4.f) Save the best model thus far\n",
    "        \n",
    "        # Printing info in this epoch\n",
    "        print(\"Epoch: {}/{}.. \".format(epoch+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "              \"Validation Loss: {:.3f}.. \".format(valid_losses[-1]),\n",
    "              \"Validation Accuracy: {:.3f}\".format(accuracy/len(valid_loader)))\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return train_losses, valid_losses\n",
    "\n",
    "# Doing the training loop - you may want to play with the number of epochs to train\n",
    "train_losses, valid_losses = train(model, criterion, optimizer, train_loader, valid_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the losses\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Congrats! You have now trained your model. However, you still need to test it and check its generalization capability. Since we don't necessarily want to use our latest model parameters, you can use `load_model(model, checkpoint)` to instantiate your model with the best parameters found during the training process.\n",
    "\n",
    "Without further ado, implement the testing function below by following the TODO's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.1. Implement the test function\n",
    "def test(model, test_loader, criterion, checkpoint='./checkpoints/checkpoint.pth'):\n",
    "    \n",
    "    # TODO 5.1.a) Load the best model parameters\n",
    "    model = None\n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for images, labels in test_loader:\n",
    "            # TODO 5.1.b) Compute the test loss and accuracy\n",
    "            accuracy = None\n",
    "        \n",
    "        # Printing the results\n",
    "        print(\"Test Loss: {:.3f} ..\".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "\n",
    "# Doing the actual testing\n",
    "model = Network(input_dim, hidden_dims, output_dim)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if your satisfied with the generalization capabilities of your model you can used it to make predictions. At your disposal are some real (standalone) images acquired from the web. The images are located in the `datasets` folder and come in different shapes and sizes. The names of these images are numbers in the interval 0 - 7. Hence, aside from the usual `ToTensor()` and `Normalize()` transformations, you need to bring these images in the format expected by your model. To do this you may find useful the  [`Grayscale()`](https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=grayscale#torchvision.transforms.Grayscale) and [`Resize()`](https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=resize#torchvision.transforms.Resize) transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "from helper import view_classify\n",
    "\n",
    "image = PIL.ImageOps.invert(Image.open('./datasets/0.jpg').convert(\"RGB\"))\n",
    "\n",
    "# TODO 5.2.a) Create the appropriate transform\n",
    "transform = None\n",
    "\n",
    "# TODO 5.2.b) Apply the transformation\n",
    "fashion_image = None\n",
    "\n",
    "# TODO 5.2.c) Instantiate the model\n",
    "model = None\n",
    "\n",
    "# TODO 5.2.d) Load its best parameters\n",
    "model = None\n",
    "\n",
    "# TODO 5.2.e) Apply the model to fashion_image and save results in log_ps\n",
    "# Note: You still have to tell pytorch not compute gradients  \n",
    "# and put your model in eval mode   \n",
    "\n",
    "#     log_ps = None\n",
    "\n",
    "# Here's how your prediction looks like   \n",
    "ps = torch.exp(log_ps)\n",
    "print('Class distribution: ',np.around(ps.numpy(), 3))\n",
    "# Plot the image and probabilities\n",
    "view_classify(fashion_image, ps, 'FashionMNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
