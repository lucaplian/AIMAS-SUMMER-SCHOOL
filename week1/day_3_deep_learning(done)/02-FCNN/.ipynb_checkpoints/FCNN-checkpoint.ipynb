{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory 06: Fully Connected Neural Networks\n",
    "\n",
    "Now you are going to build your own deep neural network using fully connected layers in PyTorch. You are going to walk through all the steps necessary to build and train the neural network such that in can correctly classify fashion items. All the imports required for the implementation have been done for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, without further ado let's put your knowledge to the test :)\n",
    "\n",
    "**Note:** To make things predictible across multiple runs you may want to use `torch.manual_seed(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - The ETL Again\n",
    "\n",
    "Implement an simple ETL process by using FashionMNIST data set and PyTorch functionality with the following steps:\n",
    "\n",
    "1. Create a transform object that contains a normalization transformation with $\\mu = 0.5$ and $\\sigma = 0.5$.\n",
    "2. Create instances of the FashionMNIST *training set* in `train_set`  and *test set* in `test_set` with a corresponding loader objects: `train_loader` and `test_loader`.\n",
    "3. Use the objects created to print the test and train dataset: lengths, targets bincount, the shape of image batch and the the shape of targets in the batch.\n",
    "4. Plot a grid of images in the first batch with 10 images per row.\n",
    "\n",
    "**Note:** You can use any batch size you want. But, I recommend that you use a value that is greater or equal to 32 and also shuffle you data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(21)\n",
    "\n",
    "# TODO 1.1 Define a transform to normalize the data with mean 0.5 and std. 0.5\n",
    "transform = transforms.Compose([\n",
    "\n",
    "])\n",
    "\n",
    "# TODO 1.2 Download and load the FashionMNIST training/test data\n",
    "#   - root='./datasets/'\n",
    "#   - batch_size=64\n",
    "#   - shuffle=true\n",
    "train_set = None\n",
    "test_set = None\n",
    "\n",
    "train_loader = None\n",
    "test_loader = None\n",
    "\n",
    "# TODO 1.3 Print information tensor information: len/ bincount / shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Building an FC-Network Model\n",
    "\n",
    "Build a the neural network architecture presented in the Figure below by extending nn.Module class.\n",
    "\n",
    "\n",
    "<img src=\"res/fashion-nn.png\" width=600px>\n",
    "\n",
    "1. Implement the neural net constructor\n",
    "2. Implement the forward method using the log softmax probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    # TODO 2.1. Implement the NN constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        self.fc4 = None\n",
    "        \n",
    "    # TODO 2.2. Implement the forward method\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = None\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Training Your Model\n",
    "\n",
    "As in the class notes in oreder to train the model we need four main components:\n",
    "- the dataloader which we already created in exercise 1\n",
    "- an instance of our model from the class we have implemented in exercise 2\n",
    "- a loss function appropiate to our model, here we will use the negative log loss, i.e. `NLLLoss()`\n",
    "- an optimizier like SGD - here we will use Adam()\n",
    "\n",
    "Aside from these components, we also have to chose hyperparameters such as the learning rate, number of epochs to train the model and so on. Here, we'll set only the two parameters, so we can focus our efforts mainly on implemeting the training loop guts. However, in general you'll want to customize some or all aspects with respect to training (e.g. initialization sheme of model parameters, batch size, number of skip connections, etc ...)\n",
    "\n",
    "Ok, without further ado please follow the TODO's bellow to implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3.1. Instantiate the model\n",
    "model =  None\n",
    "\n",
    "# TODO 3.2. Instantiate a negative log loss criterion\n",
    "criterion = None\n",
    "\n",
    "# TODO 3.3. Instantiate an Adam optimizer with a 0.003\n",
    "optimizer = None\n",
    "\n",
    "# Number of epochs in our training loop\n",
    "epochs = 5\n",
    "\n",
    "# The training loop \n",
    "for e in range(epochs):\n",
    "    \n",
    "    # running loss to keep track of the training evolution    \n",
    "    running_loss = 0\n",
    "    \n",
    "    # Passing through batches inner loop\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        # TODO 3.4. Flatten images into a 784 long vector\n",
    "        images = None\n",
    "    \n",
    "        # TODO 3.5. Reset the gradinets for this pass\n",
    "        \n",
    "        \n",
    "        # TODO 3.6. Compute the model's output predictions\n",
    "        output = None\n",
    "        \n",
    "        # TODO 3.7. Compute the loss for this pass\n",
    "        loss = None\n",
    "        \n",
    "        # TODO 3.7. Compute the gradients using backprop.\n",
    "        \n",
    "        \n",
    "        # TODO 3.8 Update the model weights\n",
    "        \n",
    "        \n",
    "        # Tracking the loss for this batch\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # The loss in this epoch\n",
    "    print(f\"Training loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
